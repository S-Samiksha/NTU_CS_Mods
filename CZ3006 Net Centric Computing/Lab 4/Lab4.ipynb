{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CZ3006 Lab 4 \n",
    "Aim: Doing basic analysis of data log \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the dataframe using pandas library\n",
    "1. list the column names required \n",
    "2. import the csv adding in the column names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "colName = ['Type', 'sflow_agent_address', 'inputPort', 'outputPort', 'src_Mac', 'dst_Mac', 'ethernet_type', 'in_vlan', 'out_vlan', 'src_ip', 'dst_ip', 'IP_protocol', 'ip_tos', 'ip_ttl', 'upd_src_port/tcp_src_port', 'udp_dst_port/tcp_dst_port' , 'tcp_flags', 'packet_size', 'IP_Size', 'sampling_rate']\n",
    "SFlow = pd.read_csv('./SFlow_Data_lab4.csv', header = None, names=colName, index_col=False)\n",
    "SFlow = SFlow.loc[SFlow['Type'] == 'FLOW']\n",
    "SFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXERCISE 4A: TOP TALKERS AND LISTENERS\n",
    "\n",
    "One of the most commonly used function in analyzing data log is finding out the IP address of the hosts that send out large amount of packet and hosts that receive large number of packets, usually know as TOP TALKERS and LISTENERS.  Based on the IP address we can obtained the organization who owns the IP address. \n",
    "\n",
    "\n",
    "The organizations are found using the link https://whatismyipaddress.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aim is to find the top 5 talkers using IP address\n",
    "#table1 = SFlow.loc[SFlow['Type'] == 'FLOW']\n",
    "table1 = SFlow\n",
    "table1 = pd.DataFrame(table1['src_ip'])\n",
    "Top5Talkers = table1.value_counts() #by default the resulting object will be in descending order\n",
    "Top5Talkers.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aim is to find the top 5 listeners using IP address\n",
    "#table1 = SFlow.loc[SFlow['Type'] == 'FLOW']\n",
    "table1 = SFlow\n",
    "table1 = pd.DataFrame(table1['dst_ip'])\n",
    "Top5Listeners= table1.value_counts() #by default the resulting object will be in descending order\n",
    "Top5Listeners.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXERCISE 4B: TRANSPORT PROTOCOL \n",
    "\n",
    "Using the IP protocol type attribute, determine the percentage of TCP and UDP protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table1 = pd.DataFrame(SFlow['IP_protocol']) #take out IP_protocol only\n",
    "IPprotocol = table1.value_counts() #count the number of unique occurrences, by default it is in descending order\n",
    "IPprotocol = IPprotocol.to_frame().reset_index() #reset the index\n",
    "IPprotocol.columns = [\"IP_Protocol\", \"packets\"] #rename the column\n",
    "print(IPprotocol) #print the table\n",
    "totalpackets = IPprotocol['packets'].sum() #sum the total number of packets\n",
    "print()\n",
    "# printing all the statistics\n",
    "print(\"Total number of packets sent : \", totalpackets)\n",
    "print(\"Percentage of UDP\", 10*\" \", \": \", np.array(IPprotocol.loc[IPprotocol['IP_Protocol'] == 17])[0][1]/totalpackets*100)\n",
    "print(\"Percentage of TCP\", 10*\" \", \": \", np.array(IPprotocol.loc[IPprotocol['IP_Protocol'] == 6])[0][1]/totalpackets*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXERCISE 4C: APPLICATIONS PROTOCOL\n",
    "\n",
    "Using the Destination IP port number determine the most frequently used application protocol.\n",
    "(For finding the service given the port number https://www.adminsub.net/tcp-udp-port-finder/ )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table1 = pd.DataFrame(SFlow['udp_dst_port/tcp_dst_port']) #table1 extracting from main SFlow\n",
    "Top5Apps = table1.value_counts() #count the unique values e.g. 6 and 17 in descending \n",
    "Top5Apps = Top5Apps.to_frame().reset_index() #reset the index to make it continuous\n",
    "Top5Apps.columns = [\"udp_dst_port/tcp_dst_port\", \"packets\"] #renaming the columns \n",
    "print(Top5Apps.head(5)) #printing the top 5\n",
    "totalpackets = Top5Apps['packets'].sum() #sum the total number of packets sent by all application protocols\n",
    "print()\n",
    "print(\"Total number of packets sent : \", totalpackets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXERCISE 4D: TRAFFIC \n",
    "\n",
    "The traffic intensity is an important parameter that a network engineer needs to monitor closely to determine if there is congestion. You would use the IP packet size to calculate the estimated total traffic over the monitored period of 15 seconds. (Assume the sampling rate is 1 in 2048)  \n",
    "\n",
    "IP packet size corresponds to the IP_Size column name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table1 = pd.DataFrame(SFlow['IP_Size']) #table1 extracting from main SFlow\n",
    "totalbytes = table1.sum() \n",
    "totalMB = totalbytes* 2048 / 1024 /1024 #the sampling rate is one in 2048 hence multiply by 2048\n",
    "# multiply by 10^6 to change from bytes to MB\n",
    "print(\"Total Traffic (MB) : \", np.array(totalMB)[0]) #print result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXERCISE 4E: ADDITIONAL ANALYSIS \n",
    "\n",
    "Please append ONE page to provide additional analysis of the data and the insight it provides.\n",
    "Examples include:\n",
    "Top 5 communication pairs; \n",
    "Visualization of communications between different IP hosts;\n",
    "etc.\n",
    "Please limit your results within one page (and any additional results that fall beyond one page limit will not be assessed). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the top 5 communication pair\n",
    "\n",
    "We have to take into account the fact that src --> dst and dst --> src both have to be counted as a communication pair. For example, two such entries: \n",
    "\n",
    "src             |      dst\n",
    "137.132.228.15     193.62.192.8  \n",
    "193.62.192.8       137.132.228.15\n",
    "\n",
    "\n",
    "means between the IP addresses `137.132.228.15` and `193.62.192.8`   there have been **2** communications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating two tables with src_ip and dst_ip\n",
    "table1 = pd.DataFrame(SFlow[['src_ip', 'dst_ip']])\n",
    "table2 = pd.DataFrame(SFlow[['src_ip', 'dst_ip']])\n",
    "\n",
    "#renaming the columns to reverse the IP address for the scenario explained in the markdown above\n",
    "table1.rename(columns = {'dst_ip': 'IP1', 'src_ip': 'IP2'}, inplace = True)\n",
    "table2.rename(columns = {'dst_ip': 'IP2', 'src_ip': 'IP1'}, inplace = True)\n",
    "\n",
    "finaltable = pd.concat([table1, table2]) #concatenate the table \n",
    "\n",
    "#group by IP1 AND IP2 together count them, sort them in descending and reset the index\n",
    "commpair = (finaltable.groupby(['IP1', 'IP2'])).size().sort_values(ascending=False).reset_index(name='count')\n",
    "\n",
    "#print(commpair) --> results appear here itself but to make it neater ----------------------------------------------\n",
    "\n",
    "commpair['index'] = commpair.index #create another column for index to be used to remove swapped duplicates\n",
    "#split commpair table by IP1 and IP2 keeping index and count \n",
    "df_1 = commpair[['IP1', 'index', 'count']]\n",
    "df_2 = commpair[['IP2', 'index', 'count']]\n",
    "\n",
    "#rename such that they have the same column names \n",
    "df_1.columns = ['IP', 'index', 'count']\n",
    "df_2.columns = ['IP', 'index', 'count']\n",
    "\n",
    "#Keep track of which table they originally came from\n",
    "df_1['source'] = 1\n",
    "df_2['source'] = 2\n",
    "\n",
    "#concatenate them back \n",
    "df = pd.concat([df_1, df_2])\n",
    "\n",
    "#sort them by index dropping the duplicates \n",
    "out = df.sort_values(['index']).drop_duplicates(['IP'], keep='first')\n",
    "\n",
    "#extract them based on their source of origin \n",
    "df_1_out = out[out['source'] == 1][['IP', 'count', 'index']]\n",
    "df_2_out = out[out['source'] == 2][['IP', 'count', 'index']]\n",
    "\n",
    "#finally merge them and drop the index column and count_1 column\n",
    "final = df_1_out.merge(df_2_out, on='index', suffixes=('_1', '_2')).drop('index', axis=1)\n",
    "final = final.drop(columns = ['count_1'])\n",
    "\n",
    "#rename the count column \n",
    "final.rename(columns = {'count_2': 'count'}, inplace=True)\n",
    "\n",
    "#view the top 5 commmunication pairs\n",
    "print(final.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing these communication pairs (By IP Address)\n",
    "There are two parts to this:\n",
    "1. Visualising based on the IP address of the sending and receiving hosts\n",
    "2. Based on the location where these IP addresses are originating from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "commpair = final.head(500) #extract the top 500\n",
    "'''\n",
    "after index 466, it is only one time communication but for a more whole number and \n",
    "to include some one time communication 500 was chosen\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pyvis\n",
    "# Module used for the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyvis.network import Network #import the necessary libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "commgraph = Network(notebook = True)  # create an empty graph\n",
    "\n",
    "#create two tables one for IP1 and one for IP2. This is because some IP can ONLY be present in IP2 or IP1 \n",
    "iphost = pd.DataFrame(commpair[['IP_1']])\n",
    "iphost2 = pd.DataFrame(commpair[['IP_2']])\n",
    "\n",
    "#rename the columns \n",
    "iphost.rename(columns = {'IP_1': 'IP'}, inplace = True)\n",
    "iphost2.rename(columns = {'IP_2': 'IP'}, inplace = True)\n",
    "\n",
    "#concatenate the tables \n",
    "finaltable = pd.concat([iphost, iphost2])\n",
    "\n",
    "#drop duplicate IP addresses and reset the index \n",
    "finaltable = finaltable.drop_duplicates('IP').reset_index(drop=True)\n",
    "\n",
    "#print(finaltable)\n",
    "\n",
    "for i in finaltable.index:\n",
    "    commgraph.add_node(str(finaltable.loc[i, \"IP\"])) #adding nodes to the empty graph\n",
    "\n",
    "#commgraph.show('nodes.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# the list of edges is stored in commpair\n",
    "for i in commpair.index:\n",
    "    '''\n",
    "    E.g. Index 0 of the communication pair \n",
    "    IP1             |  IP2          | count\n",
    "    137.132.228.15     193.62.192.8   4951\n",
    "    between 137.132.228.15 and 193.62.192.8 we add an edge and we add a weight (called value) of 4951\n",
    "    continue for the rest of the rows\n",
    "    '''\n",
    "    commgraph.add_edge(str(commpair.loc[i, \"IP_1\"]), str(commpair.loc[i, \"IP_2\"]), value = int(commpair.loc[i, \"count\"]))\n",
    "    \n",
    "commgraph.show_buttons(filter_=True) #for more interactive display of the graph\n",
    "commgraph.show('nodes.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing these communication pairs (By Location)\n",
    "There are two parts to this:\n",
    "1. Visualising based on the IP address of the sending and receiving hosts\n",
    "2. Based on the location where these IP addresses are originating from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install IP2Location\n",
    "import os\n",
    "import IP2Location\n",
    "\n",
    "#country binary file and documentation taken from https://www.ip2location.com/developers/python\n",
    "database = IP2Location.IP2Location(os.path.join(\"IP-COUNTRY.BIN\")) \n",
    "\n",
    "#test with one ip address whether the code in documentation works\n",
    "ip = '137.132.228.15'\n",
    "rec = database.get_all(ip)\n",
    "print(rec.country_long)\n",
    "print(rec.country_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locgraph = Network(notebook = True) #creating an empty graph\n",
    "for i in finaltable.index:\n",
    "    rec = database.get_all(str(finaltable.loc[i, \"IP\"])) #get location data of IP address\n",
    "    #make sure IP address corresponds to the right country and it is available in the binary file\n",
    "    if (rec.country_long != 'INVALID IP ADDRESS' and rec.country_long != 'IPV6 ADDRESS MISSING IN IPV4 BIN' and rec.country_long != '-'):\n",
    "        locgraph.add_node(str(rec.country_long)) #add the node to empty graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# the list of edges is stored in commpair\n",
    "for i in commpair.index:\n",
    "    rec1 = database.get_all(str(commpair.loc[i, \"IP_1\"])) #get country of first IP\n",
    "    city1 = str(rec1.country_long) #convert to string\n",
    "    rec2 = database.get_all(str(commpair.loc[i, \"IP_2\"])) #get country of second IP\n",
    "    city2 = str(rec2.country_long) #convert to string\n",
    "   \n",
    "    if (city1 != 'INVALID IP ADDRESS' and city1 != 'IPV6 ADDRESS MISSING IN IPV4 BIN' and city1 != '-'):\n",
    "        if (city2 != 'INVALID IP ADDRESS' and city2 != 'IPV6 ADDRESS MISSING IN IPV4 BIN' and city2 != '-'):\n",
    "            #print(city1, city2)\n",
    "            locgraph.add_edge(city1, city2, value = int(commpair.loc[i, \"count\"])) #add the edge to the graph\n",
    "\n",
    "            \n",
    "#show the graph\n",
    "locgraph.repulsion(node_distance=70, spring_length=250)\n",
    "locgraph.show_buttons(filter_=True)\n",
    "locgraph.show('locgraph.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualization by ISP\n",
    "For this part, only the top 200 communications were taken. Due to excessive requests to the API, it frequently resulted in timeout error. Also,  in fact we only need the top 10 or 20  to see the ISPs that have the highest communication. This is because we are focused on looking at high network traffic in order to avoid congestion. \n",
    "\n",
    "However, the graph is NOT included in the submitted word document due to constraints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "# the list of edges is stored in commpair\n",
    "ispgraph = Network(notebook = True)\n",
    "for i in range(200):\n",
    "    #request the isp of the ip address from http://ip-api.com\n",
    "    rec1 = requests.get(str(\"http://ip-api.com/csv/\"+ str(commpair.loc[i, \"IP_1\"]) + \"?fields=isp\"))\n",
    "    rec2 = requests.get(str(\"http://ip-api.com/csv/\"+ str(commpair.loc[i, \"IP_2\"]) + \"?fields=isp\"))   \n",
    "    ispgraph.add_node(str(rec1.text))  #add node of IP1\n",
    "    ispgraph.add_node(str(rec2.text))  #add node of IP2\n",
    "    ispgraph.add_edge(str(rec1.text), str(rec2.text), value = int(commpair.loc[i, \"count\"])) #Add edge between IP1 and IP2 ISPs\n",
    "   \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the graph\n",
    "ispgraph.repulsion(node_distance=100, spring_length=200)\n",
    "ispgraph.show_buttons(filter_=True)\n",
    "ispgraph.show('ispgraph.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
